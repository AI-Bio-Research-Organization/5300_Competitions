# -*- coding: utf-8 -*-
"""korean_review_decode_gemma_2b.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BAsdEEnyZQ0k1X7E_bk7nUEuDS2vVLoJ

# -*- coding: utf-8 -*-
'''Optimized Korean Review Decoding Script
   Adjusted for Training Time Reduction on Colab with T4 GPU.
'''

jupyter notebook --NotebookApp.allow_origin='https://colab.research.google.com' --port=8888 --no-browser

"""

# Install dependencies (if needed in Colab)
# !pip install msal bitsandbytes msal_extensions
# !pip install -U --upgrade accelerate transformers

import gc
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from io import StringIO
import pandas as pd
from tqdm import tqdm
from torch.utils.data import Dataset
from msal import PublicClientApplication
from msal_extensions import FilePersistence, PersistedTokenCache
import requests
import os
import sys
from transformers import logging

# Dataset Loader Class
class ReviewDataset(Dataset):
    def __init__(self, dataframe):
        self.data = dataframe

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        return self.data.iloc[idx]['input'], self.data.iloc[idx]['output']

# Optimize text processing
def remove_repeated_phrases(text: str) -> str:
    return "".join(dict.fromkeys(list(text))).replace("model\n", "")

def get_ms_token(client_id, authority, scopes):
    # 캐시 파일 경로 설정
    cache_file_path = os.path.expanduser('~/.msal_cache.json')

    # MSAL Extensions를 사용하여 파일 기반 캐시 생성
    persistence = FilePersistence(cache_file_path)
    token_cache = PersistedTokenCache(persistence)

    # MSAL 앱 생성
    app = PublicClientApplication(client_id, authority=authority, token_cache=token_cache)

    # 캐시에서 기존 계정 확인
    accounts = app.get_accounts()
    if accounts:
        # 첫 번째 계정 선택 (여러 계정이 있을 경우 적절히 선택)
        result = app.acquire_token_silent(scopes, account=accounts[0])
        if 'access_token' in result:
            print('캐시된 토큰을 사용합니다.')
        else:
            print('캐시에서 유효한 토큰을 찾을 수 없습니다. 인증을 진행합니다...')
    else:
        # 디바이스 코드 플로우를 통한 새 인증 진행
        flow = app.initiate_device_flow(scopes=scopes)
        if 'user_code' not in flow:
            raise ValueError('디바이스 플로우 생성에 실패했습니다. 설정을 확인하세요.')
        print(f"다음 URL로 이동하여 코드를 입력하세요: {flow['verification_uri']}")
        print(f"인증 코드: {flow['user_code']}")
        result = app.acquire_token_by_device_flow(flow)

    if 'access_token' in result:
        print('인증에 성공했습니다!')
        headers = {'Authorization': f"Bearer {result['access_token']}"}
        return headers
    else:
        print('인증에 실패했습니다.')
        return None

def get_csv_files(folder_url, target_files, headers):
    """
    지정된 OneDrive 폴더에서 target_files에 해당하는 파일을 가져옵니다.
    """
    dataset_response = requests.get(f"{folder_url}/children", headers=headers).json()
    files = dataset_response.get('value', [])

    # 파일 이름과 ID 매핑
    file_ids = {file['name']: file['id'] for file in files}
    responses = {}

    # 타겟 파일 다운로드
    for file_name in target_files:
        file_id = file_ids.get(file_name)
        if file_id:
            file_url = f"{GRAPH_API_URL}/me/drive/items/{file_id}/content"
            file_response = requests.get(file_url, headers=headers)
            if file_response.status_code == 200:
                responses[file_name] = file_response.text  # Response의 내용을 저장
            else:
                print(f"Failed to download {file_name}: {file_response.status_code}")
        else:
            print(f"{file_name} not found in the specified folder.")

    return responses

# Load and preprocess data
def load_data(responses, rows_to_select):
    """
    다운로드한 데이터를 로드하고 전처리합니다.
    """
    # 파일 파싱
    train = pd.read_csv(StringIO(responses['train.csv']))
    test = pd.read_csv(StringIO(responses['test.csv']))
    submission = pd.read_csv(StringIO(responses['submission_gemma_2b.csv']))

    # '데이콘' 값 찾기
    try:
        start_index = submission[submission['output'] == '데이콘'].index[0]
    except IndexError:
        raise IndexError("Error: output 컬럼에 '데이콘' 값이 존재하지 않습니다. 실행을 중지합니다.")

    return train, test, submission, start_index

# Model and tokenizer initialization
def initialize_model(model_name='mindw96/Gemma-2-2B-it-DACON-LLM', device="cuda"):
    logging.set_verbosity_info()  # Enables detailed logging
    device_map = "auto"  # Default behavior, offloads to CPU if necessary

    try:
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.float16,
            device_map=device_map,
        )
        print("Model loaded successfully on GPU")
    except ValueError as e:
        print(f"Error: {e}")
        print("Retrying with device_map='cuda'...")
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.float16,
            device_map="cuda",
        )
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    tokenizer.pad_token = tokenizer.eos_token
    tokenizer.padding_side = 'right'
    return model, tokenizer

# Function to decode reviews
def decode_reviews(model, tokenizer, test, train, device="cuda"):
    restored_reviews = []
    samples = [{"input" : train['input'][i], "output" : train['output'][i]} for i in range(2)]
    system_prompt = f"You are a helpful assistant specializing in restoring obfuscated Korean reviews.\
					Your task is to transform the given obfuscated Korean review into a clear, correct\
					and natural-sounding Korean review that reflects its original meaning.\
					Below are examples of obfuscated Korean reviews and their restored forms:\n\n\
					Example, {samples[0]} \n {samples[1]}\
					Spacing and word length in the output must be restored to the same as in the input.\
					Do not provide any description. Print only in Korean."

    for _, row in tqdm(test.iterrows(), total=len(test)):
        query = row['input']
        messages = [
            {"role": "user", "content": '{}\ninput: {}, output:'.format(system_prompt, query)}
            ]
        # messages[0]['content'] = f"{system_prompt}\ninput: {query}, output:"
        input_ids = tokenizer.apply_chat_template(messages, return_tensors="pt", return_dict=True).to("cuda")
        # input_ids = tokenizer(messages[0]['content'], return_tensors="pt").to(device)

        outputs = model.generate(
            # input_ids['input_ids'],
            **input_ids,
            max_new_tokens=len(query),
            do_sample=False
        )

        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
        result = generated_text[len(messages[0]['content']) + 6:].strip()
        restored_reviews.append(remove_repeated_phrases(result))

    return restored_reviews

def save_results(restored_reviews, submission, start_index, headers, rows_to_select, result_file_name="submission_gemma_2b.csv"):
    """
    결과를 submission의 'output' 컬럼에 반영하고 OneDrive의 result 폴더에 저장합니다.
    """
    # 1. 'output' 컬럼 업데이트
    end_index = start_index + rows_to_select
    submission.loc[start_index:end_index - 1, 'output'] = restored_reviews

    # 2. CSV 파일로 로컬에 저장
    submission.to_csv(result_file_name, index=False, encoding='utf-8-sig')
    print(f"CSV 파일 저장 완료: {result_file_name}")

    # 3. OneDrive에 파일 업로드
    with open(result_file_name, 'rb') as file_data:
        upload_url = f"{RESULT_FOLDER_URL}:/{result_file_name}:/content"
        response = requests.put(upload_url, headers=headers, data=file_data)

    # 4. 업로드 결과 확인
    if response.status_code == 200 or response.status_code == 201:
        print("파일 업로드 성공")
    else:
        print(f"파일 업로드 실패: {response.status_code}")
        print(f"응답 메시지: {response.text}")

# Azure 앱 정보
GRAPH_API_URL = 'https://graph.microsoft.com/v1.0'
CLIENT_ID = 'ef053b61-d7f1-4942-97d4-bb79fa475a01'  # 앱 등록에서 가져온 클라이언트 ID
AUTHORITY = 'https://login.microsoftonline.com/f09a4ef3-978d-434e-89da-a29b9f9f3c32'  # 테넌트 ID 또는 'common'
SCOPES = ['Files.ReadWrite.All']  # 필요 권한 설정
DATASET_FOLDER_ID = '01UUMNEVON2CIOT46PFZHKLFI3QAGBYRUR'
RESULT_FOLDER_ID = '01UUMNEVIFQKGRKG33TJGK3SDT37KR3INA'
DATASET_FOLDER_URL = f"{GRAPH_API_URL}/me/drive/items/{DATASET_FOLDER_ID}"
RESULT_FOLDER_URL = f"{GRAPH_API_URL}/me/drive/items/{RESULT_FOLDER_ID}"
ROW_TO_SELECT = 5

model, tokenizer = initialize_model()

# Main Execution
headers = get_ms_token(CLIENT_ID, AUTHORITY, SCOPES)

# 파일 다운로드
dataset_files = get_csv_files(DATASET_FOLDER_URL, ['train.csv', 'test.csv'], headers)
result_files = get_csv_files(RESULT_FOLDER_URL, ['submission_gemma_2b.csv'], headers)

# 데이터 로드 및 전처리
train, test, submission, start_index = load_data({**dataset_files, **result_files}, ROW_TO_SELECT)

restored_reviews = decode_reviews(model, tokenizer, test.iloc[start_index: start_index + ROW_TO_SELECT], train.iloc[:2])

save_results(restored_reviews, submission, start_index, headers, ROW_TO_SELECT)